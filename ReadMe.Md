# Product Deduplication Challenge

## 1. Introduction

The goal of this task is to consolidate duplicate product entries into single, enriched records per product, ensuring uniqueness and maximizing available information. The dataset was provided as a `.parquet` file and converted to `.csv` for processing. Many products appeared multiple times with partial or overlapping attributes, requiring careful deduplication and merging.

## 2. Steps Taken

### 2.1 Dataset Analysis

I examined key fields (`name`, `brand`, `price`, `description`, `image_url`) and noted variations in spelling, formatting, and missing values. These observations guided the normalization and matching criteria.

### 2.2 Duplicate Detection

Using string normalization (removing diacritics, collapsing spaces, lowercasing) and exact or fuzzy comparison on `product_identifier`, I grouped entries that referred to the same product.

### 2.3 Merging Duplicates

For each group, I merged fields by:

- Keeping all non-null values for descriptions and image URLs.
- Resolving conflicting prices by selecting the most common value.
- Preserving maximum available data per product.

### 2.4 Backup CSV

All original rows — including those flagged as duplicates — were saved in `duplicates_backup.csv` for transparency and rollback if needed.

### 2.5 Output

The final, deduplicated dataset was written to `deduplicated_products.csv`, containing one enriched row per unique product.

## 3. Dependencies

This project uses Node.js and the following npm packages:

- `csv-parser` — parse CSV files
- `parquetjs-lite` — read `.parquet` files
- `csv-writer` — write CSV files

Install all dependencies with:

```bash
npm install
```

## 4. Technical Decisions

### 4.1 Script Separation

The task was separated into two scripts for clarity and maintainability:

- **`convert.js`**: Handles the conversion of the `.parquet` file to a `products.csv` format.
- **`deduplicate.js`**: Responsible for normalizing the data, detecting duplicates, merging records, and saving the final output.

This separation isolates data extraction from the actual deduplication process, making it easier to manage and debug.

### 4.2 `.gitignore` Configuration

The `.gitignore` file was configured to exclude unnecessary files from the repository:

- **`node_modules/`**: Contains third-party dependencies installed via npm.
- **`*.csv`**: Excludes all CSV files, including the backup and output files that are generated during processing.
- **`.DS_Store`**: Excludes macOS-specific system files.

These exclusions help keep the repository clean by not tracking files that are either autogenerated or specific to the local development environment.

## 5. Conclusion

This solution successfully consolidated 21,946 product entries into 1,806 unique product records, significantly reducing duplication while preserving valuable information. The scripts are designed to be modular, making future updates or changes straightforward.
